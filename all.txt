==> ex0/stream_processor.py <==
from abc import ABC, abstractmethod
from typing import Any


class DataProcessor(ABC):
    """Abstract base class for processing different types of data.

    This class defines the interface for data processors that can validate,
    process, and format output for various data types.
    """

    @abstractmethod
    def process(self, data: Any) -> str:
        """Process the input data and return a string result.

        Args:
            data: The data to process (type depends on implementation).

        Returns:
            A string containing the processed result.
        """
        ...

    @abstractmethod
    def validate(self, data: Any) -> bool:
        """Validate if the input data is of the correct type.

        Args:
            data: The data to validate.

        Returns:
            True if data is valid, False otherwise.
        """
        ...

    def format_output(self, result: str) -> str:
        """Format the output result.

        Args:
            result: The result string to format.

        Returns:
            The formatted result string.
        """
        return result


class NumericProcessor(DataProcessor):
    """Processor for numeric data (lists of numbers).

    Validates and processes lists of numeric values, calculating statistics
    like sum and average.
    """

    def validate(self, data: Any) -> bool:
        """Validate that the data is a list of numeric values.

        Args:
            data: The data to validate.

        Returns:
            True if data is a list of numbers, False otherwise.
        """
        try:
            len(data)
            total: int = 0
            for num in data:
                total += num
        except (ValueError, TypeError):
            print("Not list of numbers")
            return False

        print("Validation: Numeric data verified")
        return True

    def process(self, data: Any) -> str:
        """Process numeric data and calculate statistics.

        Args:
            data: A list of numeric values.

        Returns:
            A string with count, sum, and average of the values.
        """
        output: str
        try:
            output = (f"Processed {len(data)} numeric values "
                      f", sum={sum(data)}, avg={sum(data) / len(data)}")
        except (ValueError, TypeError) as e:
            output = f"Numeric Processor Error: {e}"
        return self.format_output(output)


class TextProcessor(DataProcessor):
    """Processor for text data (strings).

    Validates and processes text strings, analyzing character and word counts.
    """

    def validate(self, data: Any) -> bool:
        """Validate that the data is a string.

        Args:
            data: The data to validate.

        Returns:
            True if data is a string, False otherwise.
        """
        try:
            data.capitalize()
        except (ValueError, TypeError):
            print("Not a string")
            return False
        print("Validation: Text data verified")
        return True

    def process(self, data: Any) -> str:
        """Process text data and analyze its structure.

        Args:
            data: A string to analyze.

        Returns:
            A string with character and word count statistics.
        """
        output: str = ""

        try:
            output = (f"Processed text: {len(data)} chars,"
                      f" {len(data.split())} words")
        except (TypeError, ValueError) as e:
            output = f"Text Processor Error: {e}"
        return self.format_output(output)


class LogProcessor(DataProcessor):
    """Processor for log entries.

    Validates and processes log messages, detecting ERROR and INFO levels
    and formatting them appropriately.
    """

    def validate(self, data: Any) -> bool:
        """Validate that the data is a log entry with ERROR or INFO level.

        Args:
            data: The data to validate.

        Returns:
            True if data is a valid log entry, False otherwise.
        """
        try:
            if data.find("ERROR") > -1 or data.find("INFO") > -1:
                print("Validation: Log entry verified")
                return True
        except (AttributeError, TypeError):
            print("Not a log entry")
            return False
        return False

    def process(self, data: Any) -> str:
        """Process log data and format by severity level.

        Args:
            data: A log entry string.

        Returns:
            A formatted log message with appropriate alert level.
        """
        output: str = ""

        try:
            if data.find("ERROR") > -1:
                output = "[ALERT] ERROR level detected: " + data[7:]
            elif data.find("INFO") > -1:
                output = "[INFO] INFO level detected: " + data[6:]
        except (TypeError, AttributeError) as e:
            output = f"Log Processor Error: {e}"
        return self.format_output(output)

    def format_output(self, result: str) -> str:
        """Override the format_output method for log-specific formatting.

        Args:
            result: The result string to format.

        Returns:
            The formatted result with log prefix.
        """
        return "Overridden log:" + result


def main() -> None:
    """Main function demonstrating polymorphic data processing.

    Tests different processor implementations with various data types,
    showing validation, processing, and output formatting capabilities.
    """
    print("Initializing Numeric Processor...")
    print("Processing data: [1, 2, 3, 4, 5]")
    lst: list[int] = [1, 2, 3, 4, 5]
    processor: DataProcessor = NumericProcessor()
    if processor.validate(data=lst):
        print(processor.process(data=lst))
    # Just to check
    print("\nError check:")
    processor.validate("Hi!")

    print("")
    print("Initializing Text Processor...")
    print('Processing data: "Hello Nexus World"')
    processor = TextProcessor()
    if processor.validate("Hello Nexus World"):
        print(processor.process("Hello Nexus World"))
    # Just to check
    print("\nError check:")
    processor.validate(45)

    print("")
    print("Initializing Log Processor...")
    print('Processing data: "ERROR: Connection timeout"')
    processor = LogProcessor()
    if processor.validate("ERROR: Connection timeout"):
        print(processor.process("ERROR: Connection timeout"))
    # Just to check
    print("\nError check:")
    processor.validate(45)

    print("\n=== Polymorphic Processing Demo ===")
    print("Processing multiple data types through same interface...")
    polimorph: list[DataProcessor] = [
        NumericProcessor(),
        TextProcessor(),
        LogProcessor()
    ]

    print("Result 1: ", polimorph[0].process([6, 0, 0]))
    print("Result 2: ", polimorph[1].process("ABCDEF ABCDE"))
    print("Result 3: ", polimorph[2].process("INFO: System ready"))

    print("\n=== Override Demo. Only Log has been overrided ===")
    print("Result 1: ", polimorph[0].format_output("For numbers"))
    print("Result 2: ", polimorph[1].format_output("For texts"))
    print("Result 3: ", polimorph[2].format_output("For Log"))


if __name__ == "__main__":
    main()

==> ex1/data_stream.py <==
from abc import ABC, abstractmethod
from typing import Any, Dict  # typings are different in modern Python


class DataStream(ABC):
    """Abstract base class for processing data streams.

    Defines the interface for different types of data streams that can
    process batches, filter data, and provide statistics.
    """

    def __init__(self, stream_id: str) -> None:
        """Initialize stream metadata and counters.

        Args:
            stream_id: Unique identifier for the stream.
        """
        self.processed_count: int = 0
        self.stream_id: str = stream_id

    @abstractmethod
    def process_batch(self, data_batch: list[Any]) -> str:
        """Process a batch of data and return a summary string.

        Args:
            data_batch: Items to process.

        Returns:
            A string summary of the processed batch.
        """
        pass

    def filter_data(
        self,
        data_batch: list[Any],
        criteria: str | None = None) -> list[Any]:
        """Return a filtered batch based on optional criteria.

        Args:
            data_batch: Items to filter.
            criteria: Optional filter criteria.

        Returns:
            The filtered list of items.
        """
        return data_batch

    def get_stats(self) -> Dict[str, str | int | float]:
        """Return basic stream statistics.

        Returns:
            A dictionary containing stream_id and processed_count.
        """
        return {
            "stream_id": self.stream_id,
            "processed_count": self.processed_count
        }


class SensorStream(DataStream):
    """Stream processor for sensor data readings.

    Processes numerical sensor readings, calculates averages,
    and provides filtering by high/standard values.
    """

    def __init__(self, stream_id: str) -> None:
        """Initialize a sensor stream.

        Args:
            stream_id: Unique identifier for the stream.
        """
        super().__init__(stream_id)

    def process_batch(self, data_batch: list[Any]) -> str:
        """Process sensor readings and report the average.

        Args:
            data_batch: Sensor readings to process.

        Returns:
            A string summary with reading count and average.
        """
        try:
            average = sum(data_batch) / len(data_batch)
            self.processed_count += len(data_batch)
            return (
                f"[{self.stream_id}] "
                f"{len(data_batch)} readings processed, avg: {average:.2f}"
            )

        except (TypeError, ZeroDivisionError):
            return f"[{self.stream_id}] Invalid sensor data"

    def filter_data(
                    self,
                    data_batch: list[Any],
                    criteria: str | None = None) -> list[Any]:
        """Filter sensor readings by criteria.

        Args:
            data_batch: Sensor readings to filter.
            criteria: Filter criteria for readings ("high" or "standard").

        Returns:
            The filtered list of sensor readings.
        """
        try:
            if criteria == "high":
                return [data for data in data_batch if data > 30]
            elif criteria == "standard":
                return [data for data in data_batch if data <= 30]
        except TypeError:
            return data_batch
        return data_batch

    def get_stats(self) -> Dict[str, str | int | float]:
        """Return basic stream statistics with personalized text.

        Returns:
            A dictionary containing sensor-prefixed stream_id and
            processed_count.
        """
        return {
            "stream_id": "Sensor: " + self.stream_id,
            "processed_count": self.processed_count
        }


class TransactionStream(DataStream):
    """Stream processor for financial transaction data.

    Processes transaction values, calculates net flow,
    and provides filtering by positive/negative values.
    """

    def __init__(self, stream_id: str) -> None:
        """Initialize a transaction stream.

        Args:
            stream_id: Unique identifier for the stream.
        """
        super().__init__(stream_id)

    def process_batch(self, data_batch: list[Any]) -> str:
        """Process transactions and report the net flow.

        Args:
            data_batch: Transaction values to process.

        Returns:
            A string summary with operation count and net flow.
        """
        try:
            total = sum(data_batch)
            self.processed_count += len(data_batch)
            return (
                f"[{self.stream_id}] "
                f"{len(data_batch)} operations processed, net flow: {total}"
            )

        except TypeError:
            return f"[{self.stream_id}] Invalid transaction data"

    def filter_data(
                    self,
                    data_batch: list[Any],
                    criteria: str | None = None
                ) -> list[Any]:
        """Filter transactions by criteria.

        Args:
            data_batch: Transaction values to filter.
            criteria: Filter criteria for transactions
                ("positive" or "negative").

        Returns:
            The filtered list of transactions.
        """

        if criteria is None:
            return data_batch
        try:
            if criteria == "positive":
                return [data for data in data_batch if data >= 0]
            elif criteria == "negative":
                return [data for data in data_batch if data < 0]
        except (TypeError, ValueError):
            return data_batch

        return data_batch

    def get_stats(self) -> Dict[str, str | int | float]:
        """Return basic stream statistics with personalized text.

        Returns:
            A dictionary containing transaction-prefixed stream_id and
            processed_count.
        """
        return {
            "stream_id": "Transactions: " + self.stream_id,
            "processed_count": self.processed_count
        }


class EventStream(DataStream):
    """Stream processor for event log data.

    Processes event entries, counts errors,
    and provides filtering by error/info events.
    """

    def __init__(self, stream_id: str) -> None:
        """Initialize an event stream.

        Args:
            stream_id: Unique identifier for the stream.
        """
        super().__init__(stream_id)

    def process_batch(self, data_batch: list[Any]) -> str:
        """Process events and report error counts.

        Args:
            data_batch: Event entries to process.

        Returns:
            A string summary with event count and error count.
        """
        try:
            total = len(data_batch)
            error_count = sum(
                1 for event in data_batch
                if isinstance(event, str) and "error" in event
            )
            self.processed_count += total
            return (
                f"[{self.stream_id}] "
                f"{total} events processed, {error_count} error(s)"
            )

        except TypeError:
            return f"[{self.stream_id}] Invalid event data"

    def filter_data(
        self,
        data_batch: list[Any],
        criteria: str | None = None) -> list[Any]:
        """Filter events by criteria.

        Args:
            data_batch: Event entries to filter.
            criteria: Filter criteria for events ("error" or "info").

        Returns:
            The filtered list of events.
        """

        if criteria is None:
            return data_batch

        try:
            if criteria == "error":
                return [data for data in data_batch if "error" in data]
            elif criteria == "info":
                return [data for data in data_batch if "error" not in data]
        except (TypeError, ValueError):
            return data_batch

        return data_batch

    def get_stats(self) -> Dict[str, str | int | float]:
        """Return basic stream statistics with personalized text.

        Returns:
            A dictionary containing event-prefixed stream_id and
            processed_count.
        """
        return {
            "stream_id": "Events: " + self.stream_id,
            "processed_count": self.processed_count
        }


class StreamProcessor:
    """Coordinates processing and filtering across multiple data streams.

    Manages a collection of DataStream instances and orchestrates
    batch processing and filtering operations.
    """

    def __init__(self, streams: list[DataStream]) -> None:
        """Initialize the processor with streams.

        Args:
            streams: Streams to process.
        """
        self.streams: list[DataStream] = streams

    def process_streams(
                        self,
                        data_map: Dict[str, list[Any]]) -> None:
        """Process all streams using the provided data map.

        Args:
            data_map: Mapping of stream IDs to data batches.

        Returns:
            None. Results are printed to stdout.
        """

        for stream in self.streams:
            batch: list[Any] = data_map.get(stream.stream_id, [])
            try:
                result: str = stream.process_batch(batch)
                print(result)
            except Exception as e:
                print(f"Processing error in {stream.stream_id}: {e}")

    def filter_streams(
            self,
            data_map: Dict[str, list[Any]],
            criteria_map: Dict[str, str]) -> None:
        """Filter all streams using the provided criteria map.

        Args:
            data_map: Mapping of stream IDs to data batches.
            criteria_map: Mapping of stream IDs to filter criteria.

        Returns:
            None. Results are printed to stdout.
        """

        for stream in self.streams:
            batch = data_map.get(stream.stream_id, [])
            criteria = criteria_map.get(stream.stream_id)
            try:
                filtered = stream.filter_data(batch, criteria)
                print(
                    f"{stream.stream_id}: "
                    f"{len(filtered)} filtered item(s)"
                )
            except Exception as e:
                print(f"Filtering error in {stream.stream_id}: {e}")


def main() -> None:
    """Main function demonstrating stream processing functionality.

    Creates sensor, transaction, and event streams, then demonstrates
    batch processing, filtering, and statistics gathering.
    """
    data_streams: list[DataStream] = [
        SensorStream("SENSOR_001"),
        TransactionStream("TRANS_001"),
        EventStream("EVENT_001")
    ]

    data_lists: Dict[str, list[Any]] = {
        "SENSOR_001": [22.5, 50, 21.8],
        "TRANS_001": [100.00, 150.00, -75.00],
        "EVENT_001": ["login", "error", "logout"]
    }

    processor = StreamProcessor(data_streams)

    print("=== Batch Processing ===")
    processor.process_streams(data_lists)

    print("\n=== Filtering ===")

    filter_criteria: Dict[str, str] = {
        "SENSOR_001": "high",
        "TRANS_001": "negative",
        "EVENT_001": "error"
    }

    processor.filter_streams(data_lists, filter_criteria)

    print("\n=== Stream Statistics ===")
    for stream in data_streams:
        print(stream.get_stats())


if __name__ == "__main__":
    main()

==> ex2/nexus_pipeline.py <==
from abc import ABC, abstractmethod
from typing import Protocol, Any


class ProcessingStage(Protocol):
    """Does not create a real class, only defines an interface.
    Any class with method 'process' will be valid as ProcessingStage
    """

    def process(self, data: Any) -> Any:
        """Do not use 'pass' because it is not real.
        pass is also valid, becuse it creates an empty method.
        '...' ex more explicit telling that this define only the signature,
        does not create a class or method"""
        ...


class InputStage:
    def process(self, data: Any) -> Any:
        print("Stage Input: Validating and parsing data")
        return data


class TransformStage:
    def process(self, data: Any) -> Any:
        print("Stage Transform: Enriching data")
        if data == "FAIL":
            raise ValueError("Invalid data format")
        return data


class OutputStage:
    def process(self, data: Any) -> Any:
        print("Stage Output: Formatting result")
        return data


class ProcessingPipeline(ABC):

    def __init__(self, pipeline_id: str) -> None:
        self.pipeline_id: str = pipeline_id
        self.stages: list[ProcessingStage] = []
        self.processed_count: int = 0

    def add_stage(self, stage: ProcessingStage) -> None:
        self.stages.append(stage)

    def run_stages(self, data: Any) -> Any:
        current_data: Any = data

        for stage in self.stages:
            try:
                current_data = stage.process(current_data)
            except Exception as e:
                print(f"Error detected in stage: {e}")
                print("Recovery initiated: Switching to safe mode")

                # Fallback strategy
                current_data = f"[RECOVERED]{current_data}"

                print("Recovery successful. Continuing pipeline.")

        return current_data

    @abstractmethod
    def process(self, data: Any) -> Any:
        """The modern way is (str | Any) instead of Union[str, Any]"""
        ...


class JSONAdapter(ProcessingPipeline):
    def process(self, data: Any) -> Any:
        print("Adapter: Processing JSON data through pipeline...")
        print(f"Receiving: {data}\n")
        result = self.run_stages(data)
        self.processed_count += 1
        return f"Processed JSON data: {result}"


class CSVAdapter(ProcessingPipeline):
    def process(self, data: Any) -> Any:
        print("Adapter: Processing CSV data through same pipeline...")
        print(f"Receiving: {data}\n")
        result = self.run_stages(data)
        self.processed_count += 1
        return f"Processed CSV data: {result}"


class StreamAdapter(ProcessingPipeline):
    def process(self, data: Any) -> Any:
        print("Adapter: Processing Stream data through same pipeline...")
        print(f"Receiving: {data}\n")
        result = self.run_stages(data)
        self.processed_count += 1
        return f"Processed STREAM data: {result}"


class NexusManager():
    def __init__(self) -> None:
        self.pipelines: list[ProcessingPipeline] = []

    def add_pipeline(self, pipeline: ProcessingPipeline) -> None:
        self.pipelines.append(pipeline)

    def execute_all(self, data: Any) -> None:
        """“Execute all registered pipelines independently.”"""
        for pipeline in self.pipelines:
            try:
                result: Any = pipeline.process(data)
                print(f"Nexus Manager result: {result}\n")
            except Exception as e:
                print(f"Error procesing data {e}")

    def execute_chain(self, data: Any) -> Any:
        """This is a pipeline.
        The output of one stage is the input of the next"""

        current_data = data
        i: int = 1
        for pipeline in self.pipelines:
            print(f"\nChain pass {i}: ")
            current_data = pipeline.process(current_data)
            i = i + 1
        return current_data


def main():
    """
    main()
    ↓
    NexusManager
    ↓
    ProcessingPipeline
    ↓
    Stages
    """

    print("=== CODE NEXUS - ENTERPRISE PIPELINE SYSTEM ===")
    print("Initializing Nexus Manager...")

    manager = NexusManager()

    print("Creating json pipeline with 3 stages")
    json_pipeline = JSONAdapter("JSON_01")
    json_pipeline.add_stage(InputStage())
    json_pipeline.add_stage(TransformStage())
    json_pipeline.add_stage(OutputStage())

    print("Creating csv pipeline with 3 stages")
    csv_pipeline = CSVAdapter("CSV_01")
    csv_pipeline.add_stage(InputStage())
    csv_pipeline.add_stage(TransformStage())
    csv_pipeline.add_stage(OutputStage())

    print("Creating stream pipeline with 3 stages")
    stream_pipeline = StreamAdapter("STREAM_01")
    stream_pipeline.add_stage(InputStage())
    stream_pipeline.add_stage(TransformStage())
    stream_pipeline.add_stage(OutputStage())

    print("Adding 3 pipelines to Pipeline Manager")
    manager.add_pipeline(json_pipeline)
    manager.add_pipeline(csv_pipeline)
    manager.add_pipeline(stream_pipeline)

    print("Using dummy data")
    input_data: str = "TEST DATA"

    print("\n=== Multi-Format Data Processing ===")
    print("Calling all pipelines. Each pipeline has 3 stages.\n")
    manager.execute_all(input_data)

    print("\n=== Statistics ===")
    for pipeline in manager.pipelines:
        print(f"Pipeline {pipeline.pipeline_id}: {pipeline.processed_count}")

    print("\n=== Pipeline Chaining Demo ===")
    print("Calling all pipelines chaining output -> input")

    final = manager.execute_chain(input_data)
    print("Chain result:", final)

    print("\n=== Statistics ===")
    for pipeline in manager.pipelines:
        print(f"Pipeline {pipeline.pipeline_id}: {pipeline.processed_count}")

    print("\n=== Error Recovery Test ===")
    print("Simulating pipeline failure...\n")

    manager.execute_all("FAIL")


if __name__ == "__main__":
    main()
